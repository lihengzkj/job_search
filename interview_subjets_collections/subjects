
25.Spark 的stage的理解


26.Spark 宽依赖和窄依赖

27.Spark 二次排序

28.Spark reduceByKey和groupByKey区别

1、RDD,DAG,Stage怎么理解？

2、宽依赖 窄依赖怎么理解？

3、Stage是基于什么原理分割task的？

4、血统的概念

5、任务的概念

6、容错方法

7、粗粒度和细粒度

8、Spark优越性

9、Spark为什么快

10、Transformation和action是什么？区别？举几个常用方法

11、RDD怎么理解

12、spark 作业提交流程是怎么样的，client和 cluster 有什么区别，各有什么作用

13、spark on yarn 作业执行流程，yarn-client 和 yarn cluster 有什么区别

14、spark streamning 工作流程是怎么样的，和 storm 比有什么区别

15、spark sql 你使用过没有，在哪个项目里面使用的

16、spark 机器学习和 spark 图计算接触过没，，能举例说明你用它做过什么吗？

17、spark rdd 是怎么容错的，基本原理是什么？

1.讲讲你做的过的项目。 项目里有哪些 难点重点注意点呢？

2.讲讲多线程吧， 要是你，你怎么实现一个线程池呢？

3.讲一下Mapreduce或者hdfs的原理和机制。map读取数据分片。

4.shuffle 是什么？ 怎么调优？

5.项目用什么语言写？ Scala？ Scala的特点？ 和Java的区别？

6.理论基础怎么样，比如数据结构，里面的快速排序，或者，树？ 讲一讲你了解的树的知识？

7.数学怎么样呢？

8.讲一下数据库，SQl ，左外连接， 原理，实现？

9.还了解过数据的什么知识？ 数据库引擎？
10.Hadoop的机架怎么配置的？
11.Hbase的设计有什么心得？
12.Hbase的操作是用的什么API还是什么工具？
13.对调度怎么理解.? 用什么工具吗？

14.用kettle 这种工具还是 自己写程序？ 你们公司是怎么做的？

15.你们数据中心开发周期是多长？
16.你们hbase里面是存一些什么数据。
---------------------

1.讲讲你做的项目。

2.平时 对多线程 这方面是怎么处理呢？ 异步 是怎么思考呢？ 遇到的一些锁啊， 是怎么做的呢？ 比如两个人同时操作一样东西。怎么做的呢？一些并发操作设计到一些变量怎么做的呢？

3.你们用的最多是 http协议吧？ 有没有特殊的头呢？ 讲讲 你对tcp/ip的理解？
4.有没有用过Zookeeper呢？ Zookeeper的适用场景是什么？ HA 状态维护 分布式锁 全局配置文件管理 操作Zookeeper是用的什么？
5.spark开发分两个方面？哪两个方面呢？

6.比如 一个读取hdfs上的文件，然后count有多少行的操作，你可以说说过程吗。那这个count是在内存中，还是磁盘中计算的呢？磁盘中。
7.spark和Mapreduce快？ 为什么快呢？ 快在哪里呢？ 1.内存迭代。2.RDD设计。 3,算子的设计。
8.spark sql又为什么比hive快呢？
10.RDD的数据结构是怎么样的？ Partition数组。 dependence
11.hadoop的生态呢。说说你的认识。 hdfs底层存储 hbase 数据库 hive数据仓库 Zookeeper分布式锁 spark大数据分析
---------------------

2.Hbase的PUT的一个过程。

3.RDD算子里操作一个外部map比如往里面put数据。然后算子外再遍历map。有什么问题吗。

4.shuffle的过程。调优。

5.5个partition里面分布有12345678910.用算子求最大值或者和。不能用广播变量和累加器。或者sortbykey.

6.大表和小表join.
7.知道spark怎么读hbase吗？spark on hbase.。华为的。
8.做过hbase的二级索引吗？
9.sort shuffle的优点？
10.stage怎么划分的？ 宽依赖窄依赖是什么？
---------------------
1.讲讲你做过的项目(一个整体思路)
2.问问大概情况。公司里集群规模。hbase数据量。数据规模。
3.然后挑选数据工厂开始详细问。问hbase.。加闲聊。
4.问二次排序是什么。topn是什么。二次排序要继承什么接口？
5.计算的数据怎么来的。
6.kakfadirect是什么，。为什么要用这个，有什么优点？。和其他的有什么区别。
---------------------

7.问了shuffle过程。
8.怎么调优的，jvm怎么调优的？
9.jvm结构？堆里面几个区？
10.数据清洗怎么做的？
11.怎么用spark做数据清洗
12.跟我聊了spark的应用，商场里广告投放，以及黄牛检测
13.spark读取 数据，是几个Partition呢？ hdfs几个block 就有几个 Partition？
14.spark on yarn的两种模式? client 模式？ 和cluster模式？
15.jdbc？mysql的驱动包名字叫什么？
16.region多大会分区？
---------------------

1.说说Mapreduce？一整个过程的理解。讲一下。
2.hbase存数据用什么rowkey？加时间戳的话，会不会出现时间戳重复的问题，怎么做的呢？
3.Spring的两大模块？ AOP，IOC在你们项目中分别是怎么用的呢？
4.你们集群的规模， 数据量？

1.画图，画Spark的工作模式，部署分布架构图
2.画图，画图讲解spark工作流程。以及在集群上和各个角色的对应关系。

3.java自带有哪几种线程池。
4.数据是怎么收集的。 kafka收集数据的原理？
5.画图，讲讲shuffle的过程。那你怎么在编程的时候注意避免这些性能问题。
6.讲讲列式存储的 parquet文件底层格式。
7.dataset和dataframe？
8.通过什么方式学习spark的？
9.有哪些数据倾斜，怎么解决？
10.宽依赖窄依赖？
11.yarn的原理？
12.BlockManager怎么管理硬盘和内存的。
13.哪些算子操作涉及到shuffle
14.看过源码？ 你熟悉哪几个部分的源码？
15.集群上 nodemanager和ResourceManager的数量关系？
16.spark怎么整合hive？ 大概这样。 spark on hive 。 hive还是hive 执行引擎是spark。
---------------------

1.Spark如何处理结构化数据，Spark如何处理非结构话数据？
2.Spark性能优化主要有哪些手段？
3.简要描述Spark分布式集群搭建的步骤
4.对于Spark你觉得他对于现有大数据的现状的优势和劣势在哪里？
5.对于算法是否进行过自主的研究设计？
6.简要描述你了解的一些数据挖掘算法与内容 基本我有印象的就这几个问题，聊了2个多小时，脑子都差点被问干了
---------------------
